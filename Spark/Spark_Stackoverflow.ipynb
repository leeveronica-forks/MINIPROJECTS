{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "StackOverflow is a collaboratively edited question-and-answer site generally\n",
    "focused on programming topics. Because of the variety of features tracked,\n",
    "including a variety of feedback metrics, it allows for some open-ended analysis\n",
    "of user behavior on the site.\n",
    "\n",
    "The data is available on [S3](s3://thedataincubator-course/spark-stack-data/)\n",
    "There are three subfolders: allUsers, allPosts, and allVotes which contain\n",
    "chunked and gzipped xml with the following format:\n",
    "\n",
    "```xml\n",
    "<row Body=\"&lt;p&gt;I always validate my web pages, and I recommend you do the same BUT many large company websites DO NOT and cannot validate because the importance of the website looking exactly the same on all systems requires rules to be broken. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general, valid websites help your page look good even on odd configurations (like cell phones) so you should always at least try to make it validate.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2008-10-12T20:26:29.397\" Id=\"195995\" LastActivityDate=\"2008-10-12T20:26:29.397\" OwnerDisplayName=\"Eric Wendelin\" OwnerUserId=\"25066\" ParentId=\"195973\" PostTypeId=\"2\" Score=\"0\" />\n",
    "```\n",
    "\n",
    "A full schema can be found\n",
    "[here](https://ia801500.us.archive.org/8/items/stackexchange/readme.txt) which\n",
    "originates from [this](https://archive.org/details/stackexchange).\n",
    "## Spark Overflow\n",
    "\n",
    "We'll use some open-source code (you'll need to clone [this\n",
    "repo](https://github.com/stevenrskelton/SparkOverflow)) to handle the\n",
    "integration of the StackOverflow xml into Scala.\n",
    "\n",
    "The scala files in the SparkOverflow project will parse the XML for you,\n",
    "allowing you to pull out tags by writing `x.creationDate`. Look through\n",
    "the scala parsing source code (eg. StackTable.scala, Post.scala) to see how\n",
    "this works. You may also want to look\n",
    "[here](http://stevenskelton.ca/files/2013/12/Real-Time-Data-Mining-With-Spark.scala)\n",
    "and [here](http://stevenskelton.ca/real-time-data-mining-spark/) to get a\n",
    "better grasp of what this code does.\n",
    "\n",
    "Also noted that add the following line to the build.sbt.\n",
    "\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upvote_percentage_by_favorites\n",
    "\n",
    "Each post on StackExchange can be upvoted, downvoted, and favorited. One\n",
    "\"sanity check\" we can do is to look at the ratio of upvotes to downvotes as a\n",
    "function of how many times the post has been favorited.  Using post favorite\n",
    "counts as the keys for your mapper, calculate the average percentage of upvotes\n",
    "(upvotes / (upvotes + downvotes)) for the first 50 keys (starting from the\n",
    "least favorited posts).\n",
    "  \n",
    "Do the analysis on the stats.stackexchange.com dataset.\n",
    "\n",
    "## user_answer_percentage_by_reputation:\n",
    "\n",
    "Investigate the correlation between a user's reputation and the kind of posts\n",
    "they make. For the 99 users with the highest reputation, single out posts which\n",
    "are either questions or answers and look at the percentage of their posts that\n",
    "are answers (answers / (answers + questions)).\n",
    "\n",
    "## user_reputation_by_tenure:\n",
    "If we use the total number of posts made on the site as a metric for tenure, we\n",
    "can look at the differences between \"younger\" and \"older\" users. You can\n",
    "imagine there might be many interesting features - for now just return the top\n",
    "100 post counts and the average reputation for every user who has that count.\n",
    "\n",
    "## quick_answers_by_hour\n",
    "How long do you have to wait to get your question answered? Look at the set of\n",
    "ACCEPTED answers which are posted less than three hours after question\n",
    "creation. What is the average number of these \"quick answers\" as a function of\n",
    "the hour of day the question was asked?  You should normalize by how many total\n",
    "accepted answers are garnered by questions posted in a given hour, just like\n",
    "we're counting how many quick accepted answers are garnered by questions posted\n",
    "in a given hour, eg. (quick accepted answers when question hour is 15 / total\n",
    "accepted answers when question hour is 15).\n",
    "\n",
    "## identify_veterans_from_first_post_stats\n",
    "It can be interesting to think about what factors influence a user to remain\n",
    "active on the site over a long period of time.  In order not to bias the\n",
    "results towards older users, we'll define a time window between 100 and 150\n",
    "days after account creation. If the user has made a post in this time, we'll\n",
    "consider them active and well on their way to being veterans of the site; if\n",
    "not, they are inactive and were likely brief users.\n",
    "\n",
    "\n",
    "Let's see if there are differences between the first ever question posts of\n",
    "\"veterans\" vs. \"brief users\". For each group separately, average the score,\n",
    "views, number of answers, and number of favorites of users' first question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "package ca.stevenskelton.sparkoverflow\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import scala.Predef._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.log4j.{ LogManager, Level }\n",
    "import org.apache.spark.SparkConf\n",
    "import scala.math\n",
    "    \n",
    "\n",
    "object Main extends App {\n",
    " //def main(args: Array[String]){\n",
    "  import java.io.File\n",
    "\n",
    "  //val sparkHome = \"C:/Program Files/Hadoop/spark-0.8.0-incubating/\"\n",
    "\n",
    "  val inputDir = args(0)\n",
    "  val outputDir = args(1)\n",
    "  val minSplits = 4\n",
    "\n",
    "  //System.getenv(\"SPARK_HOME\"),Seq(System.getenv(\"SPARK_EXAMPLES_JAR\")))\n",
    "  //LogManager.getRootLogger().setLevel(Level.WARN)\n",
    "  //System.setProperty(\"spark.worker.memory\", \"3g\")\n",
    "  System.setProperty(\"spark.executor.memory\", \"5g\")\n",
    "  System.setProperty(\"spark.rdd.compress\", \"true\")\n",
    "  //if (!true) {\n",
    "    //System.setProperty(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    //System.setProperty(\"spark.kryo.registrator\", \"ca.stevenskelton.sparkoverflow.KyroRegistrator\")\n",
    "  //}\n",
    "  \n",
    "  println(\"Start spark.\")\n",
    "  val conf = new SparkConf().setAppName(\"Main\")\n",
    "  conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .set(\"spark.default.parallelism\", \"12\")\n",
    "        .set(\"spark.hadoop.validateOutputSpecs\", \"false\")\n",
    "  conf.registerKryoClasses(Array(classOf[Post], classOf[User], classOf[Vote]))\n",
    "  val sc = new SparkContext(conf)\n",
    "\n",
    "  println(\"Load data\")\n",
    "  //LOAD DATA USING SPARK\n",
    "  val jsonData = sc.textFile(Post.file.getAbsolutePath, minSplits)\n",
    "  val objData = jsonData.flatMap(Post.parse)\n",
    "  objData.cache\n",
    "\n",
    "  //val posts = objData.keyBy(_.id)\n",
    "\n",
    "  val jsonVoteData = sc.textFile(Vote.file.getAbsolutePath, minSplits)\n",
    "  val voteData = jsonVoteData.flatMap(Vote.parse)\n",
    "  voteData.cache\n",
    "\n",
    "  val jsonUserData = sc.textFile(User.file.getAbsolutePath, minSplits)\n",
    "  val userData = jsonUserData.flatMap(User.parse)\n",
    "  userData.cache\n",
    "\n",
    "\n",
    "//=====================================*upvote_percentage_by_favorites*===================================================// \n",
    "      //val votes = voteData.map(item=>(item.voteTypeId,1)).reduceByKey(_ + _).sortByKey()    //votes is a list      \n",
    "      val votegroup_fav = voteData.map(item=>((item.postId,item.voteTypeId),1)).reduceByKey((a,b)=>a+b).filter(item=>(item._1._2==5)).map(item=>(item._1._1,item._2))\n",
    "      val votegroup_down = voteData.map(item=>((item.postId,item.voteTypeId),1)).reduceByKey((a,b)=>a+b).filter(item=>(item._1._2==3)).map(item=>(item._1._1,item._2))\n",
    "      val votegroup_up = voteData.map(item=>((item.postId,item.voteTypeId),1)).reduceByKey((a,b)=>a+b).filter(item=>(item._1._2==2)).map(item=>(item._1._1,item._2))  \n",
    "  val join_group_1 = votegroup_fav.fullOuterJoin(votegroup_up).mapValues(x=>(x._1.getOrElse(0),x._2.getOrElse(0)))\n",
    "  val join_group_2 = join_group_1.fullOuterJoin(votegroup_down).mapValues(x=>(x._1.getOrElse((0,0)),x._2.getOrElse(0)))\n",
    "  join_group_2.take(50).foreach(println)\n",
    "  val fav_downup = join_group_2.map(item=>(item._2._1._1,item._2._1._2*1.0/(item._2._1._2+item._2._2))).sortByKey(false)\n",
    "  println(fav_downup.mapValues(item=>(item,1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2)).mapValues(item=>item._1*1.0/item._2).sortByKey().take(50).mkString(\",\"))*/\n",
    "\n",
    "  \n",
    "      \n",
    "//=====================================*user_answer_percentage_by_reputation*===================================================// \n",
    "            \n",
    "      val user_rep = userData.map(x=> (x.id,x.reputation))\n",
    "      val post_answer = objData.map(x=>((x.ownerUserId,x.postTypeId),1)).reduceByKey((a,b)=>a+b).filter(x=>(x._1._2==2)).map(x=>(x._1._1,x._2))\n",
    "      val post_question = objData.map(x=>((x.ownerUserId,x.postTypeId),1)).reduceByKey((a,b)=>a+b).filter(x=>(x._1._2==1)).map(x=>(x._1._1,x._2))\n",
    "      \n",
    "      val group1 = user_rep.fullOuterJoin(post_answer).mapValues(x=>(x._1.getOrElse(0),x._2.getOrElse(0)))\n",
    "      val group2 = group1.fullOuterJoin(post_question).mapValues(x=>(x._1.getOrElse((0,0)),x._2.getOrElse(0))).map(x=>(x._2._1._1,(x._1,x._2._1._2,x._2._2)))\n",
    "      \n",
    "      val rep_percent = group2.map(x=>(x._1,(x._2._1,x._2._2*1.0/(x._2._2+x._2._3)))).sortByKey(false).take(99).map(x=>(x._2._1,x._2._2))\n",
    "           \n",
    "      println(rep_percent.mkString(\",\"))\n",
    "      \n",
    "      \n",
    "      \n",
    "      //=====================================*user_reputation_by_tenure*===================================================// \n",
    "      \n",
    "      val user_rep = userData.map(x=> (x.id,x.reputation))\n",
    "      val post_count = objData.map(x=>(x.ownerUserId,1)).reduceByKey((a,b)=>a+b)\n",
    "      val rep_count = user_rep.join(post_count).map(x=>(x._2._2,x._2._1)).sortByKey(false).mapValues(item=>(item,1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2)).mapValues(item=>item._1*1.0/item._2)\n",
    "      println(rep_count.sortByKey(false).take(100).mkString(\",\"))\n",
    "      \n",
    "      \n",
    "      \n",
    "      //=====================================*quick_answers_by_hour*===================================================// \n",
    "            \n",
    "      val aaRdd = objData.filter(x=>x.postTypeId == 1).map(x=>(x.acceptedAnswerId,(x.creationhour,x.creationDate)))\n",
    "      val creationdateRdd = objData.map(x=>(x.id,x.creationDate))\n",
    "      val joingroup = aaRdd.join(creationdateRdd).map(x=>(x._2._1._1,(x._2._1._2,x._2._2)))\n",
    "      \n",
    "      val quick_answer = joingroup.filter(x=>(x._2._2-x._2._1)<=3600000*3).map(x=>(x._1,1)).reduceByKey(_+_)\n",
    "      val total_answer = joingroup.map(x=>(x._1,1)).reduceByKey(_+_)\n",
    "      val final_group = quick_answer.join(total_answer).map(x=>(x._1,x._2._1*1.0/x._2._2)).sortByKey()\n",
    "      println(final_group.collect().mkString(\",\"))      \n",
    "      \n",
    "\n",
    "      //=====================================*identify_veterans_from_first_post_stats*===================================================//      \n",
    "      val user_create_rdd = userData.map(x=>(x.id,x.creationDate))\n",
    "      val post_create_rdd = objData.map(x=>(x.ownerUserId,x.creationDate)).sortBy(_._2)//.reduceByKey((x,y)=>math.min(x,y))\n",
    "      val joingroup_veteran = user_create_rdd.join(post_create_rdd).map(x=>(x._1,x._2._2-x._2._1)).filter(x=>x._2<=100*24*3600000L || x._2>=150*24*3600000L).reduceByKey((x,y)=>(x)).map(x=>(x._1,1))   // find the veteran users\n",
    "        \n",
    "      val user_first_question = objData.map(x=>(x.ownerUserId,(x.postTypeId,x.creationDate))).filter(x=>(x._2._1==1)).map(x=>(x._1,x._2._2)).reduceByKey((x,y)=>math.min(x,y))  // find the first question for the every user\n",
    "      val joingroup_firstquestion = joingroup_veteran.join(user_first_question).map(x=>((x._2._2,x._1),1))   // find the first question for veteran users ( first_question_creationdate,veteran_userid)\n",
    "      val post_attribute_rdd = objData.map(x=>((x.creationDate,x.ownerUserId),(x.viewCount,x.score,x.favoriteCount,x.answerCount)))\n",
    "      \n",
    "      val result_part1 = joingroup_firstquestion.join(post_attribute_rdd).map(x=>(1,(x._2._2._1 , x._2._2._2 , x._2._2._3 , x._2._2._4 , 1))).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2,x._3+y._3,x._4+y._4,x._5+y._5)).mapValues(x=>(x._1*1.0/x._5,x._2*1.0/x._5,x._3*1.0/x._5,x._4*1.0/x._5))   \n",
    "      println(result_part1.collect().mkString(\"\"))\n",
    "      \n",
    "        \n",
    " //}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala",
   "name": "scala-2.10"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
