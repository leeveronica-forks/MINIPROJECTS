{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Graph\n",
    "\n",
    "[New York Social Diary](http://www.newyorksocialdiary.com/) provides a\n",
    "fascinating lens onto New York's socially well-to-do.  The data forms a natural\n",
    "social graph for New York's social elite.  Take a look at this page of a recent\n",
    "run-of-the-mill holiday party:\n",
    "\n",
    "`http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers`\n",
    "\n",
    "Besides the brand-name celebrities, you will notice the photos have carefully\n",
    "annotated captions labeling those that appear in the photos.  We can think of\n",
    "this as implicitly implying a social graph: there is a connection between two\n",
    "individuals if they appear in a picture together.\n",
    "\n",
    "Fetching the data can be broken down into two phases:\n",
    "\n",
    "The first step is to crawl the data.  We want photos from parties before\n",
    "December 1st, 2014.  Go to `http://www.newyorksocialdiary.com/party-pictures`\n",
    "to see a list of (party) pages.  For each party's page, grab all the captions.\n",
    "\n",
    "## Hints\n",
    "  1. Click on the on the index page and see how they change the url.  Use this\n",
    "     to determine a strategy to get all the data.\n",
    "  2. Notice that each party has a date on the index page.  Use python's\n",
    "     `datetime.strptime` function to parse it.\n",
    "  3. Some captions are not useful: they contain long narrative texts that\n",
    "     explain the event.  Usually in two stage processes like this, it is better\n",
    "     to keep more data in the first stage and then filter it out in the second\n",
    "     stage.  This makes your work more reproducible.  It's usually faster to\n",
    "     download more data than you need now than to have to redownload more data\n",
    "     later.\n",
    "  4. To avoid having to re-scrape every time you run your code, you can\n",
    "\t consider saving the data to disk, and having the parsing code load a file.\n",
    "\t A checkpoint library like\n",
    "\t [ediblepickle](https://pypi.python.org/pypi/ediblepickle/1.1.3) can\n",
    "     streamline the process so that the time-consuming code will only be run\n",
    "     when necessary.\n",
    "  5. HTTP requests can sometimes fail inconsistently. You should expect to\n",
    "     run into this issue and deal with it as best you can.\n",
    "\n",
    "Now that you have a list of all captions, you should probably save the data on\n",
    "disk so that you can quickly retrieve it.  Now comes the parsing part.\n",
    "  1. Some captions are not useful: they contain long narrative texts that\n",
    "     explain the event.  Try to find some heuristic rules to separate captions\n",
    "     that are a list of names from those that are not.  A few heuristics\n",
    "     include:\n",
    "      - look for sentences (which have verbs) and as opposed to lists of nouns.\n",
    "        For example, [nltk does part of speech\n",
    "        tagging](http://www.nltk.org/book/ch05.html) but it is a little slow.\n",
    "        There may also be heuristics that accomplish the same thing.\n",
    "      - Look for commonly repeated threads (e.g. you might end up picking up\n",
    "        the photo credits or people such as \"a friend\").\n",
    "      - Long captions are often not lists of people.  The cutoff is subjective,\n",
    "        but for grading purposes, *set that cutoff at 250 characters*.\n",
    "  2. You will want to separate the captions based on various forms of\n",
    "     punctuation.  Try using `re.split`, which is more sophisticated than\n",
    "     `string.split`.\n",
    "     **Note**: The reference solution uses regex exclusively for name parsing.\n",
    "  3. You might find a person named \"ra Lebenthal\".  There is no one by this\n",
    "     name.  Can anyone spot what's happening here?\n",
    "  4. This site is pretty formal and likes to say things like \"Mayor Michael\n",
    "     Bloomberg\" after his election but \"Michael Bloomberg\" before his election.\n",
    "     Can you find other ('optional') titles that are being used?  They should\n",
    "     probably be filtered out b/c they ultimately refer to the same person:\n",
    "     \"Michael Bloomberg.\"\n",
    "  4. There is a special case you might find where couples are written as eg.\n",
    "     \"John and Mary Smith\". You will need to write some extra logic to make\n",
    "     sure this properly parses to two names: \"John Smith\" and \"Mary Smith\".\n",
    "  5. When parsing names from captions, it can help to look at your output\n",
    "     frequently and address the problems that you see coming up, iterating\n",
    "     until you have a list that looks reasonable. This is the approach used\n",
    "     in the reference solution. Because we can only asymptotically approach\n",
    "     perfect identification and entity matching, we have to stop somewhere.\n",
    "\n",
    "**Further considerations (not included in solution)**\n",
    "  1. Who is Patrick McMullan and should he be included in the results? How would\n",
    "     you address this?\n",
    "  2. What else could you do to improve the quality of the graph's information?\n",
    "\n",
    "For the analysis, we think of the problem in terms of a\n",
    "[network](http://en.wikipedia.org/wiki/Computer_network) or a\n",
    "[graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29).  Any time a pair\n",
    "of people appear in a photo together, that is considered a link.  What we have\n",
    "described is more appropriately called an (undirected)\n",
    "[multigraph](http://en.wikipedia.org/wiki/Multigraph) with no self-loops but\n",
    "this has an obvious analog in terms of an undirected [weighted\n",
    "graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29#Weighted_graph).\n",
    "In this problem, we will analyze the social graph of the new york social elite.\n",
    "\n",
    "We recommend using python's `networkx` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_edge_tuple(List):\n",
    "    a = []\n",
    "    for x in List:\n",
    "        for y in List:\n",
    "            if x !=y:\n",
    "                a.append(tuple(list(set([x,y]))))\n",
    "        List.remove(x)\n",
    "    return [item for item in list(set(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = csv.DictReader(open('captiondata1.csv'))\n",
    "\n",
    "caption_list = []\n",
    "name_sublist = []\n",
    "name_list = []\n",
    "titles = ['Mr.', 'Mrs.', 'Ms.', 'Dr.', 'Mayor', 'CEO', 'M.D.', 'AMC', 'AOHT','ANDRUS', 'AOHT', 'ASF', 'ASPCA',\\\n",
    "         'ACO', 'ACC', 'ABT', 'ACandC', 'AFIPO', 'ALS', 'ALSGNY', 'AAADTs', 'AIA', 'AIS',\\\n",
    "         'Actress', 'Actresses', 'Actor', 'Actors', 'Author', 'Authors', 'Bad', 'C.B.E.', 'COO'\\\n",
    "         'Board Member','Photographs','Benefit Chairman', 'Benefit Chairmen', 'Benefit Chairs','CCBF Chairman',\\\n",
    "         'CCBF Doctors', 'CNBC', 'CUNY Chancellor', 'CSHL President', 'CSUN President', 'President', \\\n",
    "         'Vice President',  'Cardiologist', 'Miss New York', 'New York', 'COO'\\\n",
    "          'Board Member','Photographs','Benefit Chairman', 'Benefit Chairmen', 'Benefit Chairs','CCBF Chairman',\\\n",
    "         'CCBF Doctors', 'CNBC', 'CUNY Chancellor', 'CSHL President', 'CSUN President', 'President', \\\n",
    "         'Vice President',  'Cardiologist', 'Miss New York', 'New York', 'COO'\n",
    "         ]\n",
    "\n",
    "for row in input_file:    \n",
    "    caption = row[\"caption\"].split('%')\n",
    "    for caption_item in caption:\n",
    "        if len(caption_item)<250:\n",
    "            caption_item = caption_item.decode('utf-8').strip().replace('\\n',' ').replace('\\t',' ')\n",
    "            for word in titles:\n",
    "                if word in caption_item:\n",
    "                    caption_item = re.sub(word,'',caption_item)\n",
    "            caption_item = re.sub('[^A-Za-z\\,\\& \\.]+', ' ', caption_item)      # remove all the special characters\n",
    "            split_list = re.split(',|and |with |& ',caption_item)                    \n",
    "            name_sublist = filter(None, split_list)\n",
    "            name_sublist = [item.strip() for item in name_sublist]\n",
    "            # remove whitespaces strings\n",
    "            name_sublist_filter = filter(lambda name: name.strip() and len(name.split(' '))<=4 and name[0].isupper(),name_sublist) \n",
    "            if name_sublist_filter:\n",
    "                # deal with husband and wife case\n",
    "                new_list = []\n",
    "                c = []\n",
    "                for item in name_sublist_filter:\n",
    "                    if len(item.split(' ')) ==1:\n",
    "                        new_list.append(item)\n",
    "                        #print new_list\n",
    "                        continue\n",
    "                    else:\n",
    "                        last_name = item.split(' ')[-1]\n",
    "                        b = [first_name+\" \"+last_name for first_name in new_list]\n",
    "                        new_list = []\n",
    "                        c.extend(b)\n",
    "                        c.append(item)\n",
    "                name_list.append(c)\n",
    "                #print name_sublist_filter\n",
    "        caption_list.append(caption_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Draw a Graph\n",
    "G = nx.Graph()\n",
    "node_list = [item for x in name_list for item in x]\n",
    "new_node_list = list(set(node_list))\n",
    "G.add_nodes_from(new_node_list)\n",
    "list_tuple = []\n",
    "for item in name_list:\n",
    "    a = create_edge_tuple(item)\n",
    "    for x in a:\n",
    "        list_tuple.append(x)\n",
    "#print list_tuple\n",
    "for node_pair in list_tuple:\n",
    "    if G.has_edge(node_pair[0],node_pair[1]):\n",
    "        G[node_pair[0]][node_pair[1]]['weight']+=1            \n",
    "    else:\n",
    "        G.add_edge(node_pair[0],node_pair[1],weight = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. degree\n",
    "The simplest question to ask is \"who is the most popular\"?  The easiest way to\n",
    "answer this question is to look at how many connections everyone has.  Return\n",
    "the top 100 people and their degree.  Remember that if an edge of the graph has\n",
    "weight 2, it counts for 2 in the degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1\n",
    "degree_dict = G.degree()\n",
    "table_degree = pd.Series(degree_dict)\n",
    "sorted_table_degree = table_degree.order(ascending = False)\n",
    "sorted_list_degree = []\n",
    "for i in range(0,100):\n",
    "    Index = sorted_table_degree.index[i]\n",
    "    sorted_list_degree.append((str(sorted_table_degree.index[i]),sorted_table_degree[Index])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. pagerank\n",
    "A similar way to determine popularity is to look at their\n",
    "[pagerank](http://en.wikipedia.org/wiki/PageRank).  Pagerank is used for web\n",
    "ranking and was originally\n",
    "[patented](http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6285999) by\n",
    "Google and is essentially the stationary distribution of a [markov\n",
    "chain](http://en.wikipedia.org/wiki/Markov_chain) implied by the social graph.\n",
    "\n",
    "Use 0.85 as the damping parameter so that there is a 15% chance of jumping to\n",
    "another vertex at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Question 2\n",
    "pagerank_dict = nx.pagerank(G,alpha=0.85, personalization=None, max_iter=100, tol=1e-06, nstart=None, weight='weight', dangling=None)\n",
    "table_pagerank = pd.Series(pagerank_dict)\n",
    "sorted_table_pagerank = table_pagerank.order(ascending = False)\n",
    "sorted_list_pagerank = []\n",
    "for i in range(0,100):\n",
    "    Index = sorted_table_pagerank.index[i]\n",
    "    sorted_list_pagerank.append((str(sorted_table_pagerank.index[i]),sorted_table_pagerank[Index]))\n",
    "#print sorted_list_pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. best_friends\n",
    "Another interesting question is who tend to co-occur with each other.  Give\n",
    "us the 100 edges with the highest weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Gillian Miniter', 'Sylvester Miniter'), 75), (('Jamee Gregory', 'Peter Gregory'), 54), (('Bonnie Comley', 'Stewart Lane'), 51), (('Andrew Saffir', 'Daniel Benedict'), 51), (('Roric Tobin', 'Geoffrey Bradfield'), 46), (('Somers Farkas', 'Jonathan Farkas'), 40), (('Jay Diamond', 'Alexandra Lebenthal'), 37), (('Donald Tober', 'Barbara Tober'), 36), (('Martin Shafiroff', 'Jean Shafiroff'), 35), (('Chappy Morris', 'Melissa Morris'), 32), (('Campion Platt', 'Tatiana Platt'), 30), (('Chris Meigher', 'Grace Meigher'), 30), (('Lizzie Tisch', 'Jonathan Tisch'), 28), (('Peter Regna', 'Barbara Regna'), 27), (('Sessa von Richthofen', 'Richard Johnson'), 27), (('John Catsimatidis', 'Margo Catsimatidis'), 27), (('Wilbur Ross', 'Hilary Geary Ross'), 26), (('Arie Kopelman', 'Coco Kopelman'), 26), (('Deborah Norville', 'Karl Wellner'), 26), (('Elizabeth Stribling', 'Guy Robinson'), 24), (('Yaz Hernandez', 'Valentin Hernandez'), 24), (('Julia Koch', 'David Koch'), 24), (('Olivia Palermo', 'Johannes Huebl'), 23), (('Janna Bullock', 'R. Couri Hay'), 22), (('Michael Kennedy', 'Eleanora Kennedy'), 22), (('Anne Hearst McInerney', 'Jay McInerney'), 21), (('Douglas Hannant', 'Frederick Anderson'), 21), (('Arlene Dahl', 'Marc Rosen'), 20), (('Tommy Hilfiger', 'Dee Ocleppo'), 20), (('Mark gley', 'James Mischka'), 19), (('Donald Trump', 'Melania Trump'), 18), (('Clo Cohen', 'Charles Cohen'), 18), (('Mary Davidson', 'Marvin Davidson'), 17), (('Fernanda Kellogg', 'Kirk Henckels'), 17), (('Stephanie Krieger', 'Brian Stewart'), 17), (('Leonel Piraino', 'Nina Griscom'), 17), (('Sherrell Aston', 'Muffie Potter Aston'), 16), (('Laura Slatkin', 'Harry Slatkin'), 16), (('Sharon Bush', 'Jean Shafiroff'), 16), (('Al Roker', 'Deborah Roberts'), 16), (('Anna Safir', 'Eleanora Kennedy'), 15), (('Wilbur Ross', 'Hilary Ross'), 15), (('Lauren Bush', 'David Lauren'), 15), (('Diana Taylor', 'Michael Bloomberg'), 15), (('Thorne Perkin', 'Tatiana Perkin'), 15), (('Dennis Basso', 'Michael Cominotto'), 15), (('Chuck Scarborough', 'Ellen Scarborough'), 14), (('Elaine Langone', 'Ken Langone'), 14), (('Richard Steinberg', 'Renee Steinberg'), 14), (('Francine LeFrak', 'Rick Friedberg'), 14), (('Gillian Miniter', 'Serena Miniter'), 14), (('Randy Kemper', 'Tony Ingrao'), 14), (('Robert Bradford', 'Barbara Taylor Bradford'), 14), (('Keytt Lundqvist', 'Alex Lundqvist'), 13), (('Kim Taipale', 'Nicole Miller'), 13), (('CeCe Black', 'Lee Black'), 13), (('Donna Soloway', 'Richard Soloway'), 13), (('Gillian Hearst Simonds', 'Christian Simonds'), 13), (('Richard LeFrak', 'Karen LeFrak'), 13), (('Othon Prounis', 'Kathy Prounis'), 13), (('Coleman Burke', 'Susan Burke'), 13), (('Roxanne Palin', 'Dean Palin'), 13), (('Liz Peek', 'Jeff Peek'), 13), (('Alec Baldwin', 'Hilaria Baldwin'), 12), (('Isabel Toledo', 'Ruben Toledo'), 12), (('Martha Glass', 'John Glass'), 12), (('Melanie Wambold', 'John Wambold'), 12), (('Will Cotton', 'Rose Dergan'), 12), (('Richard Farley', 'Chele Chiavacci'), 12), (('Harry Kargman', 'Jill Kargman'), 12), (('Dan Lufkin', 'Cynthia Lufkin'), 12), (('Hunt Slonem', 'Liliana Cavendish'), 12), (('Larry Herbert', 'Michele Herbert'), 12), (('Caroline Murphy', 'Heather Matarazzo'), 12), (('Charlotte Ronson', 'Ali Wise'), 12), (('Todd Slotkin', 'Sharyn Mann'), 11), (('Jeff Koons', 'Justine Koons'), 11), (('Shirin von Wulffen', 'Frederic Fekkai'), 11), (('Jonathan Adler', 'Simon Doonan'), 11), (('Peter Martins', 'Darci Kistler'), 11), (('Ken Starr', 'Diane Passage'), 11), (('Daniel Benedict', 'Johannes Huebl'), 11), (('Gillian Miniter', 'Alexandra Lebenthal'), 11), (('David Yurman', 'Sybil Yurman'), 11), (('Judy Gilbert', 'Rod Gilbert'), 11), (('Ana Oliveira', 'Diana Taylor'), 11), (('Howard Sobel', 'Gayle Sobel'), 11), (('Mish Tworkowski', 'Joseph Singer'), 11), (('Juan Montoya', 'Urban Karlsson'), 11), (('Susan Magazine', 'Nicholas Scoppetta'), 11), (('Jean Shafiroff', 'Lucia Hwong Gordon'), 10), (('Bryant Gumbel', 'Hilary Gumbel'), 10), (('Lorenzo Martone', 'Marc Jacobs'), 10), (('Joe Pontarelli', 'Jane Pontarelli'), 10), (('Jon Heinemann', 'Michelle Marie Heinemann'), 10), (('Robert I. Grossman', 'MD Langone'), 10), (('Anne Ford', 'Charlotte Ford'), 10), (('Kyle MacLachlan', 'Desiree Gruber'), 10), (('Samantha Yanks', 'David Yanks'), 10), (('Carolina Herrera', 'Reinaldo Herrera'), 10)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 3\n",
    "weights = G.edges(data = True)\n",
    "L = []\n",
    "for (n1,n2,w) in weights:\n",
    "    t = (n1,n2,w['weight'])\n",
    "    L.append(t)\n",
    "df = pd.DataFrame(L, columns=['node1', 'node2', 'weight'])\n",
    "sorted_df = df.sort(['weight'],ascending = False)\n",
    "\n",
    "#print sorted_df[0:100]\n",
    "\n",
    "best_friends = []\n",
    "for name1,name2,weight in sorted_df[0:100].values:\n",
    "    best_friends.append(((str(name1),str(name2)),weight))\n",
    "print best_friends\n",
    "len(best_friends)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
