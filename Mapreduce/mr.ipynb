{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "We are going to be running mapreduce jobs on the wikipedia dataset.  The\n",
    "dataset is available (pre-chunked) on\n",
    "[s3](s3://thedataincubator-course/mrdata/simple/).\n",
    "\n",
    "To solve this, you will need to run mrjob on AWS EMR.  The data is stored on\n",
    "S3.  For development, we highly recommend you download a single chunk onto your\n",
    "computer:\n",
    "\n",
    "```\n",
    "wget https://s3.amazonaws.com/thedataincubator-course/mrdata/simple/part-00026.xml.bz2\n",
    "```\n",
    "\n",
    "Then take a small sample that is small enough that mrjob can process the it in\n",
    "a few seconds.  Your development cycle should be:\n",
    "\n",
    "  1. Get your job to work locally on the sampled chunk.  This will greatly\n",
    "  speed-up your development.\n",
    "  2. Get your job to work localy on the full chunk you downloaded.\n",
    "  3. Get your job to work on EMR for all of simple english.\n",
    "  4. Get your job to work on EMR for all of english wikipedia.\n",
    "\n",
    "By default, mrjob (when run on EMR) only uploads the mrjob python file and no\n",
    "supporting libraries.\n",
    "\n",
    "  1. You can always import members of the standard library as they come with\n",
    "  any python distribution.\n",
    "  2. EMR comes with `numpy`, `lxml` and a couple of other python libraries. Be\n",
    "     warned though, they are probably different versions than you have.\n",
    "  3. If you wish to include code from other local python files, use [tar them\n",
    "     up](https://pythonhosted.org/mrjob/guides/setup-cookbook.html#running-a-makefile-inside-your-source-dir))\n",
    "     to upload them to EMR.\n",
    "\n",
    "*Warning*: EMR has rather old software installed (e.g. `python 2.6` instead of\n",
    "2.7, `numpy 1.4` instead of 1.9. Make sure your code runs on these older\n",
    "libraries before uploading jobs.\n",
    "\n",
    "Finally, if you want to structure your mrjob code well, you will want to have\n",
    "multiple mrjobs in a single module.  As a matter of good style, we recommend\n",
    "that you write each separate mapreduce as it's own class.  Then write a wrapper\n",
    "module that defines the logic for combining steps.  You can combine multiple\n",
    "steps by overriding the [steps\n",
    "method](https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#multi-step-jobs).\n",
    "\n",
    "Here are some helpful articles on how mrjob works and how to pass parameters to\n",
    "your script:\n",
    "  - [How mrjob is\n",
    "    run](https://pythonhosted.org/mrjob/guides/concepts.html#how-your-program-is-run)\n",
    "  - [Adding passthrough\n",
    "  options](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.add_passthrough_option)\n",
    "  - [An example of someone solving similar\n",
    "  problems](http://arunxjacob.blogspot.com/2013/11/hadoop-streaming-with-mrjob.html)\n",
    "\n",
    "Finally, if you are find yourself processing a lot of special cases, you are\n",
    "probably doing it wrong.  For example, mapreduce jobs for\n",
    "`Top100WordsSimpleWikipediaPlain`, `Top100WordsSimpleWikipediaText`, and\n",
    "`Top100WordsSimpleWikipediaNoMetaData` are less than 150 lines of code\n",
    "(including generous blank lines and biolerplate code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top100_words_simple_plain\n",
    "Return a list of the top 100 words in article text (in no particular order).\n",
    "You will need to write this as two map reduces:\n",
    "\n",
    "1. The first job is similar to standard wordcount but with a few tweaks. \n",
    "   The data provided for wikipedia is in in *.xml.bz2 format.  Mrjob will\n",
    "   automatically decompress bz2.  We'll deal with the xml in the next question.\n",
    "   For now, just treat it as text.  A few hints:\n",
    "   - To split the words, use the regular expression \"\\w+\".\n",
    "   - Words are not case sensitive: i.e. \"The\" and \"the\" reference to the same\n",
    "     word.  You can use `string.lower()` to get a single case-insenstive\n",
    "     canonical version of the data.\n",
    "\n",
    "2. The second job will take a collection of pairs `(word, count)` and filter\n",
    "   for only the highest 100.  A few notes:\n",
    "   - To make the job more reusable make the job find the largest `n` words\n",
    "     where `n` is a parameter obtained via\n",
    "     [`get_jobconf_value`](https://pythonhosted.org/mrjob/utils-compat.html).\n",
    "   - We have to keep track of at most the `n` most popular words.  As long as\n",
    "     `n` is small, e.g. 100, we can keep track of the *running largest n* in\n",
    "     memory wtih a priority-queue. We suggest taking a look at `heapq`, part of\n",
    "     the Python standard library for this.  You may be asked about this data\n",
    "     structure on an interview so it is good to get practice with it now.\n",
    "   - To obtain the largest `n`, we need to first obtain the largest n elements\n",
    "     per chunk from the mapper, output them to the same key (reducer), and then\n",
    "     collect the largest n elements of those in the reducer (**Question:** why\n",
    "     does this gaurantee that we have found the largest n over the entire set?)\n",
    "     Given that we are using a priority queue, we will need to first initialize\n",
    "     it, then add it for each record, and then output the top `n` after seeing\n",
    "     each record.  For mappers, notice that these three phases correspond\n",
    "     nicely to these three functions:\n",
    "        - `mapper_init`\n",
    "        - `mapper`\n",
    "        - `mapper_final`\n",
    "     There are similar functions in the reducer.  Also, while the run method\n",
    "     to launch the mapreduce job is a classmethod:\n",
    "        ``` \n",
    "          if __name__ == '__main__': MRWordCount.run() \n",
    "        ```\n",
    "     actual objects are instantiated on the map and reduce nodes.  More\n",
    "     precisely, a separate mapper class is instantiated in each map node and a\n",
    "     reducer class is instantiated in each reducer node.  This means that the\n",
    "     three mapper functions can pass state through `self`, e.g. `self.heap`.\n",
    "     Remember that to pass state between the map and reduce phase, you will\n",
    "     have to use `yield` in the mapper and read each line in the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "import bz2\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w]+\")\n",
    "class FirstJob(MRJob):\n",
    "\n",
    "    #Job 1\n",
    "    def mapper_get_words (self,_,line):\n",
    "        print line\n",
    "        print 10 \n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(),1)\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "            \n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "    \n",
    "    # Job 2\n",
    "    def init_get_100(self):\n",
    "        self.counts = []\n",
    "\n",
    "    def mapper_gettop100(self,_, word_count_pairs):\n",
    "        heapq.heappush(self.counts, word_count_pairs)\n",
    "\n",
    "\n",
    "    def mapper_final_top100 (self):        \n",
    "        largest = heapq.nlargest(100,self.counts)\n",
    "        for count,word in largest:\n",
    "            yield ('heap',(count,word))\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.top100list = []\n",
    "\n",
    "\n",
    "    def reducer_get_top100(self,key,top100):\n",
    "        for word_count in top100:\n",
    "            heapq.heappush(self.top100list, (word_count[0],word_count[1]))\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        largest = heapq.nlargest(100,self.top100list)\n",
    "        words = [(word,  int(count)) for count,word in largest]\n",
    "        yield (None, words)      \n",
    "\n",
    "\n",
    "        \n",
    "    def steps(self):\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "                MRStep(mapper_init = self.init_get_100,\n",
    "                       mapper = self.mapper_gettop100,\n",
    "                      mapper_final = self.mapper_final_top100,\n",
    "                       reducer_init = self.reducer_init,\n",
    "                       reducer = self.reducer_get_top100,\n",
    "                       reducer_final = self.reducer_final)\n",
    "                    ]\n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    FirstJob.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top100_words_simple_text\n",
    "Notice that the words `page` and `text` make it into the top 100 words in the\n",
    "previous problem.  These are not common English words!  If you look at the xml\n",
    "formatting, you'll realize that these are xml tags.  You should parse the files\n",
    "so that tags like `<page></page>` should not be included in your total, nor\n",
    "should words outside of the tag `<text></text>`.\n",
    "\n",
    "*Hints*:\n",
    "1. Both `xml.etree.elementtree` from the Python stdlib or `lxml.etree` parse\n",
    "   xml. `lxml` is significantly faster though.\n",
    "\n",
    "2. In order to parse the text, we will have to accumulate a `<page></page>`\n",
    "   worth of data and then parse the resulting Wikipedia format string.\n",
    "\n",
    "3. Don't forget that the Wikipedia format can have multiple revisions but you\n",
    "   only want the latest one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import heapq\n",
    "import Queue\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "START_PAGE = re.compile('.*<page>*.')\n",
    "END_PAGE = re.compile('.*</page>*.')\n",
    "WORD_RE = re.compile(r\"[\\w]+\")\n",
    "class SecondJob(MRJob):\n",
    "\n",
    "    def mapper_get_page_init(self):\n",
    "        self.pages = Queue.Queue()\n",
    "        self.queue= Queue.Queue()\n",
    "\n",
    "    def mapper_get_page(self,_,line):\n",
    "        self.queue.put(line)\n",
    "        if END_PAGE.match(line):\n",
    "            page = ''\n",
    "            while not self.queue.empty():\n",
    "                page+=self.queue.get()\n",
    "            if START_PAGE.match(page):\n",
    "                yield ('page',page)\n",
    "\n",
    "    def reducer_get_page(self,key,pages):\n",
    "        for page in pages:\n",
    "            yield ('page',page)\n",
    "\n",
    "    # Job 2\n",
    "    def mapper_parser(self,key,page):\n",
    "        tree = ET.fromstring(page.encode('utf-8'))\n",
    "        text_tag = [(x.tag,x.text) for x in tree.getiterator()]\n",
    "        for tag, text in text_tag:\n",
    "            if tag==\"text\":\n",
    "                if text:\n",
    "                    for word in WORD_RE.findall(text):\n",
    "                        yield(word.lower(),1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "            \n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "    \n",
    "    # Job 3\n",
    "    \n",
    "    def init_get_100(self):\n",
    "        self.counts = []\n",
    "\n",
    "    def mapper_gettop100(self,_, word_count_pairs):\n",
    "        heapq.heappush(self.counts, word_count_pairs)\n",
    "\n",
    "\n",
    "    def mapper_final_top100 (self):        \n",
    "        largest = heapq.nlargest(100,self.counts)\n",
    "        for count,word in largest:\n",
    "            yield ('heap',(count,word))\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.top100list = []\n",
    "\n",
    "\n",
    "    def reducer_get_top100(self,key,top100):\n",
    "        for word_count in top100:\n",
    "            heapq.heappush(self.top100list, (word_count[0],word_count[1]))\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        largest = heapq.nlargest(100,self.top100list)\n",
    "        words = [(word,  int(count)) for count,word in largest]\n",
    "        yield (None, words)   \n",
    "\n",
    "    \n",
    "        \n",
    "    def steps(self):\n",
    "            return [\n",
    "                MRStep(mapper_init=self.mapper_get_page_init,\n",
    "                    mapper = self.mapper_get_page,\n",
    "                   reducer=self.reducer_get_page),\n",
    "                MRStep(mapper = self.mapper_parser,\n",
    "                       combiner = self.combiner_count_words,\n",
    "                       reducer = self.reducer_count_words),\n",
    "                MRStep(mapper_init = self.init_get_100,\n",
    "                       mapper = self.mapper_gettop100,\n",
    "                       mapper_final = self.mapper_final_top100,\n",
    "                       reducer_init = self.reducer_init,\n",
    "                       reducer = self.reducer_get_top100,\n",
    "                       reducer_final = self.reducer_final),\n",
    "                    ]\n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    SecondJob.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top100_words_simple_no_metadata\n",
    "Finally, notice that 'www' and 'http' make it into the list of top 100 words in\n",
    "the previous problem.  These are also not common English words!  These are\n",
    "clearly the url in hyperlinks.  Looking at the format of [Wikipedia\n",
    "links](http://en.wikipedia.org/wiki/Help:Wiki_markup#Links_and_URLs) and\n",
    "[citations](http://en.wikipedia.org/wiki/Help:Wiki_markup#References_and_citing_sources),\n",
    "you'll notice that they tend to appear within single and double brackets and\n",
    "curly braces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Third Job\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import heapq\n",
    "import Queue\n",
    "import xml.etree.ElementTree as ET\n",
    "import mwparserfromhell\n",
    "\n",
    "START_PAGE = re.compile('.*<page>*.')\n",
    "END_PAGE = re.compile('.*</page>*.')\n",
    "WORD_RE = re.compile(r\"[\\w]+\")\n",
    "class ThirdJob(MRJob):\n",
    "\n",
    "    def mapper_get_page_init(self):\n",
    "        self.pages = Queue.Queue()\n",
    "        self.queue= Queue.Queue()\n",
    "\n",
    "    def mapper_get_page(self,_,line):\n",
    "        self.queue.put(line)\n",
    "        if END_PAGE.match(line):\n",
    "            page = ''\n",
    "            while not self.queue.empty():\n",
    "                page+=self.queue.get()\n",
    "            if START_PAGE.match(page):\n",
    "                yield ('page',page)\n",
    "\n",
    "    def reducer_get_page(self,key,pages):\n",
    "        for page in pages:\n",
    "            yield ('page',page)\n",
    "\n",
    "    # Job 2\n",
    "    def mapper_parser(self,key,page):\n",
    "        tree = ET.fromstring(page.encode('utf-8'))\n",
    "        text_tag = [(x.tag,x.text) for x in tree.getiterator()]\n",
    "        for tag, text in text_tag:\n",
    "            if tag==\"text\":\n",
    "                if text:\n",
    "                    pure_text = re.sub(r'\\([^)]*\\)*|\\<[^>]*\\>*|\\[[^]]*\\]*|\\{[^}]*\\}*', '', text)\n",
    "                    for word in WORD_RE.findall(pure_text):\n",
    "                        yield(word.lower(),1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "            \n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "    \n",
    "    # Job 3\n",
    "    \n",
    "    def init_get_100(self):\n",
    "        self.counts = []\n",
    "\n",
    "    def mapper_gettop100(self,_, word_count_pairs):\n",
    "        heapq.heappush(self.counts, word_count_pairs)\n",
    "\n",
    "\n",
    "    def mapper_final_top100 (self):        \n",
    "        largest = heapq.nlargest(100,self.counts)\n",
    "        for count,word in largest:\n",
    "            yield ('heap',(count,word))\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.top100list = []\n",
    "\n",
    "\n",
    "    def reducer_get_top100(self,key,top100):\n",
    "        for word_count in top100:\n",
    "            heapq.heappush(self.top100list, (word_count[0],word_count[1]))\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        largest = heapq.nlargest(100,self.top100list)\n",
    "        words = [(word,  int(count)) for count,word in largest]\n",
    "        yield (None, words)  \n",
    "\n",
    "    \n",
    "        \n",
    "    def steps(self):\n",
    "            return [\n",
    "                MRStep(mapper_init=self.mapper_get_page_init,\n",
    "                    mapper = self.mapper_get_page,\n",
    "                   reducer=self.reducer_get_page),\n",
    "                MRStep(mapper = self.mapper_parser,\n",
    "                       combiner = self.combiner_count_words,\n",
    "                       reducer = self.reducer_count_words),\n",
    "                MRStep(mapper_init = self.init_get_100,\n",
    "                       mapper = self.mapper_gettop100,\n",
    "                       mapper_final = self.mapper_final_top100,\n",
    "                       reducer_init = self.reducer_init,\n",
    "                       reducer = self.reducer_get_top100,\n",
    "                       reducer_final = self.reducer_final),\n",
    "                    ]\n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    ThirdJob.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wikipedia_entropy\n",
    "The [Shannon\n",
    "entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of a\n",
    "discrete random variable with probability mass function p(x) is: \n",
    "\n",
    "    $$ H(X) = - \\sum p(x) \\log_2 p(x) $$ \n",
    "\n",
    "You can think of the Shannon entropy as the number of bits needed to store\n",
    "things if we had perfect compression.  It is also closely tied to the notion of\n",
    "entropy from physics.\n",
    "\n",
    "You'll be estimating the Shannon entropy of different Simple English and Thai\n",
    "based off of their Wikipedias. Do this with n-grams of characters, by first\n",
    "calculating the entropy of a single n-gram and then dividing by n to get the\n",
    "per-character entropy. Use n-grams of size 1, 2, 3, 4, 5, 10 and 15.  How\n",
    "should our per-character entropy estimates vary based off of n?  How should\n",
    "they vary by the size of the corpus? How much data would we need to get\n",
    "reasonable entropy estimates for each n?\n",
    "\n",
    "The data you need is available at:\n",
    "    - https://s3.amazonaws.com/thedataincubator-course/mrdata/simple/part-000*\n",
    "    - https://s3.amazonaws.com/thedataincubator-course/mrdata/thai/part-000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import heapq\n",
    "import Queue\n",
    "import xml.etree.ElementTree as ET\n",
    "import mwparserfromhell\n",
    "import math\n",
    "\n",
    "START_PAGE = re.compile('.*<page>*.')\n",
    "END_PAGE = re.compile('.*</page>*.')\n",
    "WORD_RE = re.compile(r\"[\\w]+\")\n",
    "THAI_RE = re.compile(r'[\\u0E00-\\u0EFF]+')\n",
    "class FourthJob(MRJob):\n",
    "\n",
    "    def mapper_get_page_init(self):\n",
    "        self.pages = Queue.Queue()\n",
    "        self.queue= Queue.Queue()\n",
    "\n",
    "    def mapper_get_page(self,_,line):\n",
    "        self.queue.put(line)\n",
    "        if END_PAGE.match(line):\n",
    "            page = ''\n",
    "            while not self.queue.empty():\n",
    "                page+=self.queue.get()\n",
    "            if START_PAGE.match(page):\n",
    "                yield ('page',page)\n",
    "\n",
    "    def reducer_get_page(self,key,pages):\n",
    "        for page in pages:\n",
    "            yield ('page',page)\n",
    "\n",
    "    # Job 2\n",
    "    def mapper_parser_init(self):\n",
    "        self.n=1\n",
    "\n",
    "\n",
    "    def mapper_parser(self,key,page):\n",
    "        tree = ET.fromstring(page.encode('utf-8'))\n",
    "        text_tag = [(x.tag,x.text) for x in tree.getiterator()]\n",
    "        #n_list = [1,2,3,4,5,10,15]\n",
    "        #n_list = [1,2]\n",
    "        for tag, text in text_tag:\n",
    "            if tag==\"text\":\n",
    "                if text:\n",
    "                    wikicode = mwparserfromhell.parse(text)\n",
    "                    pure_text = \" \".join(\" \".join(fragment.value.split()) for fragment in wikicode.filter_text())\n",
    "                    pure_text_ws = \" \".join(pure_text.split())\n",
    "                    for n_gram in [pure_text_ws[i:i+10] for i in range(len(pure_text_ws))]:\n",
    "                        yield (n_gram,1)\n",
    "\n",
    "    \n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "            \n",
    "    def reducer_count_words(self, word, counts):\n",
    "        item = sum(counts)\n",
    "        p1 = item*math.log(item,2)\n",
    "        yield word,(p1,item)\n",
    "\n",
    "    # Job 3\n",
    "    def mapper_entropy(self,key,vals):\n",
    "        p1,count = vals\n",
    "        yield None,(p1,count)\n",
    "\n",
    "    def reducer_entropy(self,_,vals):\n",
    "        total = 0\n",
    "        p1_sum = 0\n",
    "        for val in vals:\n",
    "            p1,count = val\n",
    "            total+=count\n",
    "            p1_sum+=p1\n",
    "        entropy = (math.log(total,2)-p1_sum/total)/10\n",
    "        yield None,entropy\n",
    "\n",
    "    \n",
    "\n",
    "    def steps(self):\n",
    "            return [\n",
    "                MRStep(mapper_init=self.mapper_get_page_init,\n",
    "                    mapper = self.mapper_get_page,\n",
    "                   reducer=self.reducer_get_page),\n",
    "                MRStep(mapper_init = self.mapper_parser_init,\n",
    "                       mapper = self.mapper_parser,\n",
    "                       combiner = self.combiner_count_words,\n",
    "                       reducer = self.reducer_count_words),\n",
    "                    MRStep(\n",
    "                        mapper = self.mapper_entropy,\n",
    "                        reducer = self.reducer_entropy),\n",
    "                    ]\n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    FourthJob.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## link_stats_simple\n",
    "Let's look at some summary statistics on the number of unique links on a page\n",
    "to other Wikipedia articles.  Return the number of articles (count), average\n",
    "number of links, standard deviation, and the 5%, 25%, median, 75%, and 95%\n",
    "quantiles.\n",
    "\n",
    "1. Notice that the library `mwparserfromhell` supports the method\n",
    "   `filter_wikilinks()`.\n",
    "2. You will need to compute these statistics in a way that requires O(1)\n",
    "   memory.  You should be able to compute the first few (i.e. non-quantile)\n",
    "   statistics exactly by looking at the first few moments of a distribution.\n",
    "   The quantile quantities can be accurately estimated by using reservoir\n",
    "   sampling with a large reservoir.\n",
    "3. If there are multiple links to the article have it only count for 1.  This\n",
    "   keeps our results from becoming too skewed.\n",
    "4. Don't forget that some (a surprisingly large number of) links have unicode!\n",
    "   Make sure you treat them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import heapq\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "START_RE = re.compile('.*<page>.*')\n",
    "END_RE = re.compile('.*</page>.*')\n",
    "import xml.etree.ElementTree as ET\n",
    "import Queue\n",
    "import mwparserfromhell\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class q5(MRJob):\n",
    "                  \n",
    "    def mapper_getpage_init(self):\n",
    "        self.queue = Queue.Queue()\n",
    "\n",
    "    def mapper_getpage(self,_,line):\n",
    "        self.queue.put(line)\n",
    "        if END_RE.match(line):\n",
    "            # empty the queue\n",
    "            page = ''\n",
    "            while not self.queue.empty():\n",
    "                page += self.queue.get()\n",
    "            if START_RE.match(page):\n",
    "                yield ('page',page)\n",
    "    \n",
    "    def reducer_getpage(self,key,pages):\n",
    "        pagecount = 0\n",
    "        for page in pages:\n",
    "            pagecount +=1\n",
    "            yield ('page',(page,pagecount))\n",
    "    \n",
    "\n",
    "        \n",
    "    def mapper_parser(self,key,val):        \n",
    "        sum_number_links = 0\n",
    "        reservoir,c = [],0\n",
    "        k = 5000\n",
    "        page, pagecount = val\n",
    "        tree = ET.fromstring(page.encode('utf-8','ignore'))\n",
    "        tagtext = [(x.tag, x.text) for x in tree.getiterator()]\n",
    "        for tag,text in tagtext:\n",
    "            if tag == 'text':\n",
    "                if text:\n",
    "                    wikilinks = mwparserfromhell.parse(text).filter_wikilinks()\n",
    "                    wikilinks = [ links.encode('utf-8','ignore') for links in wikilinks]\n",
    "                    x = len(set(wikilinks))\n",
    "                    sum_number_links += x\n",
    "                    if c < k:\n",
    "                        reservoir.append(x)\n",
    "                    else:\n",
    "                        r=random.randint(0,c-1)\n",
    "                        if r<k:reservoir[r] = x\n",
    "                    c+=1\n",
    "        yield None,(sum_number_links,pagecount,reservoir)\n",
    "      \n",
    "    def reducer_compute(self,_, vals):   \n",
    "            sum_links = 0\n",
    "            sum_sq = 0\n",
    "            page_count_list = []\n",
    "            reservoir_list = []\n",
    "            for val in vals:\n",
    "                link,pagecount,reservoir = val\n",
    "                sum_links+=link\n",
    "                sum_sq+=link**2\n",
    "                page_count_list.append(pagecount)\n",
    "                reservoir_list.append(reservoir)\n",
    "            total_page = max(page_count_list)   \n",
    "            counts = [ link_count for item in reservoir_list for link_count in item]\n",
    "            count_list = sorted(counts)\n",
    "            mean = sum_links*1.0/total_page\n",
    "            std= math.sqrt(sum_sq/total_page-mean**2)\n",
    "            yield ('page_count',int(total_page))\n",
    "            yield ('mean',mean)\n",
    "            yield ('std',std)        \n",
    "            yield ('5%',count_list[int(round(len(count_list)*0.05)-1)])\n",
    "            yield ('25%',count_list[int(round(len(count_list)*0.25)-1)])\n",
    "            yield ('median',count_list[int(round(len(count_list)*0.5)-1)])\n",
    "            yield ('75%',count_list[int(round(len(count_list)*0.75)-1)])\n",
    "            yield ('95%',count_list[int(round(len(count_list)*0.95)-1)])\n",
    "            #yield None,counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_getpage_init,\n",
    "                    mapper = self.mapper_getpage,\n",
    "                    reducer = self.reducer_getpage),\n",
    "           MRStep(\n",
    "                   mapper = self.mapper_parser,\n",
    "                   reducer=self.reducer_compute)\n",
    "                ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q5.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## link_stats_english Run On Aws\n",
    "The same thing but for all of English Wikipedia.  This is the real test of how\n",
    "well your algorithm scales!  The data is also located on\n",
    "[s3](s3://thedataincubator-course/mrdata/english/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Command: python q5.py -r emr s3://thedataincubator-course/mrdata/simple/ --output-dir=s3://thedataincubator-fellow/xiaojun-wc/q6/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## double_link_stats_simple\n",
    "Instead of analyzing single links, let's look at double links.  That is, pages\n",
    "A and C that are connected through many pages B where there is a link \n",
    "`A -> B -> C` or `C -> B -> A'. Find the list of all pairs `(A, C)` (you can\n",
    "use alphabetical ordering to break symmetry) that have the 100 \"most\"\n",
    "connections (see below for the definition of \"most\").  This should give us a\n",
    "notion that the articles `A` and `C` refer to tightly related concepts.\n",
    "\n",
    "1. This is essentially a Matrix Multiplication problem.  If the adjacency\n",
    "   matrix is denoted $M$ (where $M_{ij}$ represents the link between $i$ an\n",
    "   $j$), we are looking for the highest 100 elements of the matrix $M M$.\n",
    "\n",
    "2. Notice that a lot of Category pages (denoted \"Category:.*\") have a high link\n",
    "   count and will rank very highly according to this metric.  Wikipedia also\n",
    "   has `Talk:` pages, `Help:` pages, and static resource `Files:`.  All such\n",
    "   \"non-content\" pages (and there might be more than just this) and links to\n",
    "   them should be first filtered out in this analysis.\n",
    "\n",
    "3. Some pages have more links than others.  If we just counted the number of\n",
    "   double links between pages, we will end up seeing a list of articles with\n",
    "   many links, rather than concepts that are tightly connected.\n",
    "\n",
    "   1. One strategy is to weight each link as $\\frac{1}{n}$ where $n$ is the\n",
    "      number links on the page.  This way, an article has to spread it's\n",
    "      \"influence\" over all $n$ of its links.  However, this can throw off the\n",
    "      results if $n$ is small.\n",
    "\n",
    "   2. Instead, try weighting each link as $\\frac{1}{n+10}$ where 10 sets the\n",
    "      \"scale\" in terms of number of links above which a page becomes\n",
    "      \"significant\".  The number 10 was somewhat arbitrarily chosen but seems\n",
    "      to give reasonably relevant results.\n",
    "\n",
    "   3. This means that our \"count\" for a pair A,C will be the products of the\n",
    "      two link weights between them, summed up over all their shared\n",
    "      connections.\n",
    "\n",
    "4. Again, if there are multiple links from a page to another, have it only\n",
    "   count for 1.  This keeps our results from becoming skewed by a single page\n",
    "   that references the same page multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import heapq\n",
    "import xml.etree.ElementTree as ET\n",
    "import Queue\n",
    "import mwparserfromhell\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "START_RE = re.compile('.*<page>.*')\n",
    "END_RE = re.compile('.*</page>.*')\n",
    "nocontent = re.compile('(user:.*)|(user\\stalk:)|(template:.*)|(category:.*)|(talk:.*)|(help:.*)|(file:.*)|(special:.*)|(user_talk:)')\n",
    "\n",
    "class q7(MRJob):\n",
    "                  \n",
    "    def mapper_getpage_init(self):\n",
    "        self.queue = Queue.Queue()\n",
    "\n",
    "    def mapper_getpage(self,_,line):\n",
    "        self.queue.put(line)\n",
    "        if END_RE.match(line):\n",
    "            # empty the queue\n",
    "            page = ''\n",
    "            while not self.queue.empty():\n",
    "                page += self.queue.get()\n",
    "            if START_RE.match(page):\n",
    "                yield ('page',page)\n",
    "    \n",
    "    def reducer_getpage(self,key,pages):\n",
    "        for page in pages:\n",
    "            yield ('page',page)\n",
    "    \n",
    "\n",
    "        \n",
    "    def mapper_parser(self,key,page):        \n",
    "        tree = ET.fromstring(page.encode('utf-8','ignore'))\n",
    "        tagtext = [(x.tag, x.text) for x in tree.getiterator()]\n",
    "        try:\n",
    "            for tag,text in tagtext:\n",
    "                if tag ==\"title\":\n",
    "                    if text:\n",
    "                        if not nocontent.match(text):\n",
    "                            title = text  #extract the title of the page\n",
    "                elif tag == 'text':\n",
    "                    if text:\n",
    "                        wikilinks = mwparserfromhell.parse(text).filter_wikilinks()\n",
    "                        wikilinks = [ links.encode('utf-8','ignore') for links in wikilinks]\n",
    "                        # extract the title of the page that links connect to                  \n",
    "                        link_title_list = list(set([str(mwparserfromhell.parse(link).filter_wikilinks()[0].title) for link in wikilinks if not nocontent.match(str(mwparserfromhell.parse(link).filter_wikilinks()[0].title))]))\n",
    "                        for link in link_title_list:\n",
    "                            if link!=title:\n",
    "                                yield \"M1\",(title, link, 1.0/(len(link_title_list)+10.0))\n",
    "                                yield \"M2\",(title, link, 1.0/(len(link_title_list)+10.0))\n",
    "                            else:\n",
    "                                continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    def mapper_emit(self,key,val):\n",
    "        row,col,v = val\n",
    "        if key == \"M1\":\n",
    "            yield col,(row,v)\n",
    "        elif key==\"M2\":\n",
    "            yield row,((col,v),)\n",
    "            \n",
    "            \n",
    "    def multiply_values(self,j,values):\n",
    "        brow=[]\n",
    "        acol=[]\n",
    "        for val in values:\n",
    "            if len(val)==1:\n",
    "                brow.append(val[0])\n",
    "            else:\n",
    "                acol.append(val)\n",
    "                \n",
    "        for (bcol,bval) in brow:\n",
    "            for(arow,aval) in acol:\n",
    "                yield ((arow,bcol),aval*bval)\n",
    "    \n",
    "    def reducer_sum(self,key,vals):\n",
    "        yield key,sum(vals)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def heap_init(self):\n",
    "        self.h = []\n",
    "        \n",
    "    def mapper_add_to_head(self,key,val):\n",
    "        heapq.heappush(self.h,(val,key))\n",
    "        \n",
    "    def mapper_pop_top_100(self):\n",
    "        largest = heapq.nlargest(100,self.h)\n",
    "        for count,word in largest:\n",
    "            yield ('heap',(count,word))   \n",
    "            \n",
    "    def reducer_heap_init(self):\n",
    "        self.h = []\n",
    "        \n",
    "    def reducer_heap_count_words(self, key,word_counts):\n",
    "        for count,word in word_counts:\n",
    "            heapq.heappush(self.h, (count,word))\n",
    "    \n",
    "    def reducer_pop_top_100(self):\n",
    "        largest = heapq.nlargest(100,self.h)\n",
    "        words = [(word,  count) for count,word in largest]\n",
    "        yield (None, words) \n",
    "        \n",
    "        \n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_getpage_init,\n",
    "                    mapper = self.mapper_getpage,\n",
    "                    reducer = self.reducer_getpage\n",
    "                  ),\n",
    "           MRStep(\n",
    "                   mapper = self.mapper_parser,\n",
    "                 ),\n",
    "            MRStep(\n",
    "                   mapper = self.mapper_emit,\n",
    "                   reducer = self.multiply_values\n",
    "                  ),\n",
    "            MRStep(reducer = self.reducer_sum),\n",
    "            MRStep(mapper_init = self.heap_init,\n",
    "                   mapper = self.mapper_add_to_head,\n",
    "                   mapper_final = self.mapper_pop_top_100,\n",
    "                   reducer_init = self.reducer_heap_init,\n",
    "                   reducer = self.reducer_heap_count_words,\n",
    "                   reducer_final = self.reducer_pop_top_100),\n",
    "            #MRStep(reducer = self.reducer_tuple)\n",
    "                ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q7.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
